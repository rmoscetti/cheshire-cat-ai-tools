# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/super_client.ipynb.

# %% auto 0
__all__ = ['SuperCatClient', 'LLMSetting', 'LLMOpenAIChatConfig', 'LLMOllamaConfig', 'LLMGeminiChatConfig', 'LLMSettings']

# %% ../nbs/super_client.ipynb 2
from typing import Optional
from cheshire_cat_api.config import Config
from cheshire_cat_api import CatClient
from cheshire_cat_api.models import SettingBody
import requests
import time
from queue import Queue
import json
from tqdm.auto import tqdm


# %% ../nbs/super_client.ipynb 3
class SuperCatClient:
    """
    A Wrapper around the official client for sane handling of the websockets connections

    Uses a queue to communite with the websocket thread and blocks until a response is received.
    This is needed as there is a bug in the cat that results in tools not being executed if we make a simple POST request
    """

    def __init__(self, config: Optional[Config] = None):
        self.cat_client = CatClient(config, on_message=self.on_message)
        self.cat_client.connect_ws()
        self.wait_for_connection()
        self.queue = Queue()

        self.host = self.cat_client.memory.api_client.configuration.host

    def on_message(self, message):
        # this run on the websocket thread
        try:
            message = json.loads(message)
            if message.get("type") == "chat_token":
                return
            self.queue.put(message)

        except json.JSONDecodeError as e:
            print(f"Failed to decode message: {e}")

    def wait_for_connection(self, timeout=10):
        start_time = time.time()
        while not self.cat_client.is_ws_connected:
            time.sleep(1)
            if time.time() - start_time > timeout:
                raise TimeoutError(
                    f"Failed to connect to WebSocket within timeout ({timeout} sec)."
                )

    def send(self, message):
        self.cat_client.send(message)
        return self.queue.get(10)

    def udpate_setting(self, name, value, category=""):
        setting_id = next(
            (
                s["setting_id"]
                for s in requests.get(
                    f"{self.host}/settings/",
                ).json()["settings"]
                if s["name"] == name
            )
        )
        r = requests.put(
            f"{self.host}/settings/{setting_id}",
            json={
                "name": name,
                "value": value,
                'category': category
            },
        )
        r.raise_for_status()
        return r 
    
    def udpate_llm_setting(self, llm_name, value):

        r = requests.put(
            f"{self.host}/llm/settings/{llm_name}",
            json=value
        )
        r.raise_for_status()
        return r
    

    def put_sentence(self, sentence: str):
        """Put sentence in declarative memory"""
        url = f'{self.host}/memory/collections/declarative/points'
        json_data = {
            "content": sentence,
            "metadata": {}
        }
        response = requests.post(url, json=json_data)
        response.raise_for_status()
        return response

    def put_sentences(self, sentences: list[str]):
        results = []
        for sentence in tqdm(sentences, desc="Adding to declarative memory"):
            result = self.put_sentence(sentence)
            results.append(result)
        return results


    def wipe_declarative_memory(self):
        return self.cat_client.memory.wipe_single_collection("declarative")
    
    def wipe_episodic_memory(self):
        r = requests.delete(f"{self.host}/memory/conversation_history")
        r.raise_for_status()
        return r

    def __getattr__(self, name):
        # forward all the other calls to the official client
        if hasattr(self, "cat_client"):
            return getattr(self.cat_client, name)

    def close(self):
        self.cat_client.close()

    def __del__(self):
        self.cat_client.close()

    def __enter__(self):
        """Enter the runtime context related to this object."""
        return self

    def __exit__(self, exc_type, exc_value, traceback):
        """Exit the runtime context and clean up resources."""
        self.close()

# %% ../nbs/super_client.ipynb 7
from pydantic import BaseModel, SecretStr
from pydantic_settings import BaseSettings, SettingsConfigDict
from typing import ClassVar
from pyprojroot import here

# %% ../nbs/super_client.ipynb 8
class LLMSetting(BaseModel):
    pass

# %% ../nbs/super_client.ipynb 9
class LLMOpenAIChatConfig(LLMSetting):
    name: ClassVar[str] = "LLMOpenAIChatConfig"
    openai_api_key: str
    model_name: str = "gpt-5-mini"
    temperature: float = 1.0
    streaming: bool = False

# %% ../nbs/super_client.ipynb 11
class LLMOllamaConfig(LLMSetting):
    name: ClassVar[str] = "LLMOllamaConfig"
    base_url: str
    model: str = "llama3"
    num_ctx: int = 2048
    repeat_last_n: int = 64
    repeat_penalty: float = 1.1
    temperature: float = 1.0

# %% ../nbs/super_client.ipynb 12
class LLMGeminiChatConfig(LLMSetting):
    name: ClassVar[str] = "LLMGeminiChatConfig"
    google_api_key: str
    model: str = "gemini-2.5-pro-latest"
    temperature: float = 1.0
    top_p: int = 1
    top_k: int = 1
    max_output_tokens: int = 29000

# %% ../nbs/super_client.ipynb 13
class LLMSettings(BaseSettings):
    model_config = SettingsConfigDict(env_nested_delimiter="__", env_file=here(".env.local"))
    openai: LLMOpenAIChatConfig
    ollama: LLMOllamaConfig
    gemini: LLMGeminiChatConfig
