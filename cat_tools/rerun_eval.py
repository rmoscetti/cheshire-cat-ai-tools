# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/rerun-evaluation.ipynb.

# %% auto 0
__all__ = ['weave_client', 'parse_calls', 'get_dataset', 'get_missing_df', 'run_missing']

# %% ../nbs/rerun-evaluation.ipynb 2
from cat_tools.eval import (
    similarity_scorer,
    conf_variants,
    prepare_declarative_memory,
    CatModel,
)
import weave
import pandas as pd
from tqdm.auto import tqdm
import polars as pl
from polars import col as c
from .client import SuperCatClient
from datetime import datetime

# %% ../nbs/rerun-evaluation.ipynb 3
weave_client = weave.init("smart-drying-unitus/declarative-eval")

# %% ../nbs/rerun-evaluation.ipynb 5
def parse_calls(calls):
    outs = []
    for call in tqdm(list(calls)):
        model_name = call.inputs["model"].name
        has_mem = "NO memory" not in call.display_name
        for row in call.children():
            out = {}
            if inp := row.inputs.unwrap():
                if inp := inp.get("example", False):
                    out["question_id"] = inp["id"]
                    out["question"] = inp["prompt"]
                    out["expected_answer"] = inp["target"]
            if output := row.output:
                output = output.unwrap()
                if "output" in output:
                    if "text_clean" in output["output"]:
                        out["model_answer"] = output["output"]["text_clean"]
                    if "scores" in output:
                        if "CatEmbeddingSimilarityScorer" in output["scores"]:
                            out["similarity_score"] = output["scores"][
                                "CatEmbeddingSimilarityScorer"
                            ]["similarity_score"]

            out["model_name"] = model_name
            out["weave_call"] = row
            out["has_declarative_memory"] = has_mem
            outs.append(out)
    return pd.DataFrame(outs)

# %% ../nbs/rerun-evaluation.ipynb 22
def get_dataset(missing):
    datasets = []
    for row in missing.iter_rows(named=True):
        dataset = []
        for id, q, t in zip(row["id"], row["question"], row["target"]):
            if id is None:
                continue
            dataset.append({"id": id, "prompt": q, "input": q, "target": t})
        datasets.append(dataset)
    return datasets

# %% ../nbs/rerun-evaluation.ipynb 24
def get_missing_df(weave_client, query=calls_query):
    calls = list(
        weave_client.get_calls(
            filter={"trace_roots_only": True},
            query=query,
            sort_by=[{"field": "started_at", "direction": "desc"}],
        )
    )
    df = parse_calls(calls)
    df_pl = pl.from_pandas(df.drop(columns=["weave_call"]))
    missing = (
        df_pl.filter(c.question_id.is_not_null())
        .group_by(
            c.model_name,
            c.has_declarative_memory,
            c.question,
            c.expected_answer,
            c.question_id,
        )
        .agg(
            all_missing=c.model_answer.is_null().all(),
        )
        .filter(c.all_missing)
        .group_by(c.model_name, c.has_declarative_memory)
        .agg(
            question=c.question,
            target=c.expected_answer,
            id=c.question_id.cast(pl.Int64),
            n=pl.len(),
        )
        .sort(c.has_declarative_memory)
    )
    missing = missing.with_columns(
        dataset=pl.Series(get_dataset(missing), dtype=pl.Object)
    )
    return missing

# %% ../nbs/rerun-evaluation.ipynb 25
async def run_missing(missing):
    client = SuperCatClient()
    time = datetime.now().strftime("%m-%d %H:%M")
    eval_name = f"{time} Eval"
    cat_has_memory = False
    for row in missing.iter_rows(named=True):
        dataset = row["dataset"]
        model_name = row["model_name"]
        has_mem = row["has_declarative_memory"]
        if cat_has_memory != has_mem:
            if has_mem:
                print("Preparing declarative memory...")
                prepare_declarative_memory(client)
            else:
                client.wipe_declarative_memory()
            cat_has_memory = has_mem
        print(f"Running {model_name} with memory={has_mem} on {len(dataset)} examples")
        model = CatModel(
            model_name, conf_variants[model_name], has_declarative_memory=has_mem
        )
        evaluation = weave.Evaluation(
            dataset=dataset,
            scorers=[similarity_scorer],
            name=eval_name,
        )
        await evaluation.evaluate(
            model,
            __weave={
                "display_name": f"{eval_name} - {model_name} {'memory' if has_mem else 'NO memory'}"
            },
        )
